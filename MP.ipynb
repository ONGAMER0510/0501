{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "v6O64CQN8MsK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "count=0\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "v7izN6Um5Dwg"
      },
      "outputs": [],
      "source": [
        "in_fight=r'.\\SCVD\\Videos\\Fight'\n",
        "out_fight=r'.\\SCVD\\Frames\\Fight'\n",
        "for filename in os.listdir(in_fight):\n",
        "  #print(filename)\n",
        "  vidObj = cv2.VideoCapture(in_fight+fr'\\{filename}') \n",
        "  while True: \n",
        "        success, image = vidObj.read() \n",
        "        #print(success)\n",
        "        # Saves the frames with frame-count \n",
        "        if success:\n",
        "          cv2.resize(image,(240,240))\n",
        "          cv2.imwrite(fr\"{out_fight}\\frame{count}.jpg\", image) \n",
        "          count += 1\n",
        "        else:\n",
        "          break\n",
        "  vidObj.release() \n",
        "  cv2.destroyAllWindows() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IiElyjBTZqal"
      },
      "outputs": [],
      "source": [
        "count=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "P8T3yQ-b8JZZ"
      },
      "outputs": [],
      "source": [
        "In_non_fight=r'SCVD\\Videos\\Non_Fight'\n",
        "Out_non_fight=r'SCVD\\Frames\\Non_Fight'\n",
        "for filename in os.listdir(In_non_fight):\n",
        "  #print(filename)\n",
        "  vidObj = cv2.VideoCapture(In_non_fight+fr\"\\{filename}\") \n",
        "  while True: \n",
        "        success, image = vidObj.read() \n",
        "        #print(success)\n",
        "        # Saves the frames with frame-count \n",
        "        if success:\n",
        "          cv2.resize(image,(240,240))\n",
        "          cv2.imwrite(fr\"{Out_non_fight}\\frame{count}.jpg\", image) \n",
        "          count += 1\n",
        "        else:\n",
        "          break\n",
        "  vidObj.release() \n",
        "  cv2.destroyAllWindows() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "HSyi1SZvFRww",
        "outputId": "5372bfec-c632-4b54-fe22-e442aa5d898f"
      },
      "outputs": [],
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
        "#from data import DataSet\n",
        "import os.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C0p63fXvXHHq"
      },
      "outputs": [],
      "source": [
        "def get_model(weights='imagenet'):\n",
        "    # create the base pre-trained model\n",
        "    base_model = InceptionV3(weights=weights, include_top=False)\n",
        "\n",
        "    # add a global spatial average pooling layer\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # let's add a fully-connected layer\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    # and a logistic layer\n",
        "    predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "    # this is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-vhPShG2XVDA"
      },
      "outputs": [],
      "source": [
        "def get_generators():\n",
        "    # train_datagen = ImageDataGenerator(\n",
        "    #     rescale=1./255,\n",
        "    #     shear_range=0.2,\n",
        "    #     horizontal_flip=True,\n",
        "    #     rotation_range=10.,\n",
        "    #     width_shift_range=0.2,\n",
        "    #     height_shift_range=0.2)\n",
        "\n",
        "    # test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.5,\n",
        "        rotation_range=25)\n",
        "    \n",
        "    test_datagen = ImageDataGenerator(rescale=1. / 255,validation_split=0.5)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        r'C:\\Science\\NAZAR Raw\\Train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        subset='training',\n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "    \n",
        "    validation_generator = test_datagen.flow_from_directory(\n",
        "        r'C:\\Science\\NAZAR Raw\\Train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        subset='validation',\n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "    return train_generator,validation_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7QP9emMuX2WK"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "generators = get_generators()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zcQCLfJPJtc2"
      },
      "outputs": [],
      "source": [
        "checkpointer = ModelCheckpoint(\n",
        "    filepath=os.path.join('data', 'checkpoints', 'inception.{epoch:03d}.hdf5'),\n",
        "    verbose=1,\n",
        "    save_best_only=True)\n",
        "\n",
        "# Helper: Stop when we stop learning.\n",
        "early_stopper = EarlyStopping(patience=10)\n",
        "\n",
        "# Helper: TensorBoard\n",
        "tensorboard = TensorBoard(log_dir=os.path.join('data', 'logs'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rQ2CkkyGYBFT"
      },
      "outputs": [],
      "source": [
        "def train_model(model, nb_epoch, generators, callbacks=[]):\n",
        "    train_generator,val_generator =generators\n",
        "    model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=100,\n",
        "        epochs=nb_epoch,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=100,\n",
        "        callbacks=callbacks)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "InntPv3TbMRt"
      },
      "outputs": [],
      "source": [
        "def freeze_all_but_top(model):\n",
        "    \"\"\"Used to train just the top layers of the model.\"\"\"\n",
        "    # first: train only the top layers (which were randomly initialized)\n",
        "    # i.e. freeze all convolutional InceptionV3 layers\n",
        "    for layer in model.layers[:-2]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # compile the model (should be done *after* setting layers to non-trainable)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def freeze_all_but_mid_and_top(model):\n",
        "    \"\"\"After we fine-tune the dense layers, train deeper.\"\"\"\n",
        "    # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "    # the first 172 layers and unfreeze the rest:\n",
        "    for layer in model.layers[:172]:\n",
        "        layer.trainable = False\n",
        "    for layer in model.layers[172:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # we need to recompile the model for these modifications to take effect\n",
        "    # we use SGD with a low learning rate\n",
        "    model.compile(\n",
        "        optimizer=SGD(lr=0.0001, momentum=0.9),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "colab_type": "code",
        "id": "uKGzI1iEbUH5",
        "outputId": "2793b919-09ea-4e15-ace6-d7b545baf8c4"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "model = freeze_all_but_top(model)\n",
        "model = train_model(model, 10, generators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "colab_type": "code",
        "id": "DhBWjvigbbNt",
        "outputId": "b65d69f1-6fe9-4efc-a9f1-bac220aee87d"
      },
      "outputs": [],
      "source": [
        "for file in os.listdir(r'.\\SCVD\\Frames'):\n",
        "  print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "Mk6WFoqrb0Px",
        "outputId": "8249691d-e9e0-46c6-c204-61f7e4e122aa"
      },
      "outputs": [],
      "source": [
        "import shutil \n",
        "\n",
        "location=r\"C:\\Science\\Violence-Detection-in-Videos-master\\fight-detection-surv-dataset-master\\Dataset\"\n",
        "path = os.path.join(location, '.ipynb_checkpoints') \n",
        "print(path)\n",
        "# removing directory \n",
        "shutil.rmtree(path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "colab_type": "code",
        "id": "cvqo17FhcRJD",
        "outputId": "c7db1c04-fa61-4308-86f0-45f7a6c94556"
      },
      "outputs": [],
      "source": [
        "for file in os.listdir('/content/fight-detection-surv-  dataset/Dataset'):\n",
        "  print('/content/fight-detection-surv-dataset/Dataset/'+file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "colab_type": "code",
        "id": "2_GBvp-9dUzj",
        "outputId": "debf31bd-7e54-483d-dc49-62e193a84e7a"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "model = freeze_all_but_mid_and_top(model)\n",
        "model = train_model(model, 10, generators,\n",
        "                        [checkpointer, early_stopper, tensorboard])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KGGLs4UvwP5h"
      },
      "outputs": [],
      "source": [
        "count=0\n",
        "#for filename in os.listdir('/content/fight-detection-surv-dataset/noFight'):\n",
        "  #print(filename)\n",
        "vidObj = cv2.VideoCapture(r'C:\\Science\\Violence-Detection-in-Videos-master\\fight-detection-surv-dataset-master\\Dataset\\V10.mp4') \n",
        "while True: \n",
        "        success, image = vidObj.read() \n",
        "        #print(success)\n",
        "        # Saves the frames with frame-count \n",
        "        if success:\n",
        "          cv2.resize(image,(299,299))\n",
        "          cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\frames\\frame{count}.jpg\", image) \n",
        "          count += 1\n",
        "        else:\n",
        "          break\n",
        "vidObj.release() \n",
        "cv2.destroyAllWindows() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now() # current date and time\n",
        "\n",
        "date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
        "model.save(r'.\\saved\\model0301full.h5')\n",
        "print('Model Saved!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "model= keras.models.load_model(r'.\\saved\\model0301half.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8U7mfriryXZw"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "Z6NYVFg5weSa",
        "outputId": "029872d7-27dd-4c65-ae4f-cb3e1942c961"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "img=1\n",
        "fight=0\n",
        "non_fight=0\n",
        "inp=input(\"Enter a room number\")\n",
        "cap=cv2.VideoCapture()\n",
        "cap.open(f'rtsp://admin:sgis12345@172.16.16.132:554/Streaming/Channels/{str(inp)}01/')\n",
        "success,frame=cap.read()\n",
        "print(frame)\n",
        "while success:\n",
        "  status=''\n",
        "  success,frame=cap.read()\n",
        "  image=frame\n",
        "  if image is not None:\n",
        "      # # x = image.img_to_array(img)\n",
        "      # x = np.expand_dims(image, axis=0)\n",
        "      # images = np.vstack([x])\n",
        "      # classes = model.predict(images, batch_size=10)\n",
        "      # print(classes)\n",
        "      # if classes[0][1]<=1/(10**30):\n",
        "      #   status=\"FAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"\n",
        "      #   cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Fight{fight}.jpg\", image)\n",
        "      #   fight+=1\n",
        "      # else:\n",
        "      #   cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\non_fight\\nonFight{non_fight}.jpg\", image)\n",
        "      #   status=\"No fight\"\n",
        "      #   non_fight+=1\n",
        "      # cv2.putText(image,status, (0,69), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)\n",
        "      cv2.namedWindow(\"Violence Detection\", cv2.WINDOW_NORMAL)\n",
        "      cv2.resizeWindow(\"Violence Detection\", 1366, 768)\n",
        "      cv2.imshow(\"Violence Detection\",image)\n",
        "      cv2.waitKey(1)\n",
        "      print(\"frame\"+\" \"+status)\n",
        "      img+=1 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img=1\n",
        "fight=0\n",
        "non_fight=0\n",
        "cap=cv2.VideoCapture(0)\n",
        "success,frame=cap.read()\n",
        "while success:\n",
        "  status=''\n",
        "  success,frame=cap.read()\n",
        "  image=frame\n",
        "  if image is not None:\n",
        "      #x = image.img_to_array(img)\n",
        "      x = np.expand_dims(image, axis=0)\n",
        "      images = np.vstack([x])\n",
        "      classes = model.predict(images, batch_size=10)\n",
        "      print(classes)\n",
        "      if classes[0][0]>0.85:\n",
        "        status=\"|||||||||||||||||||||||||||||||||\"\n",
        "        cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Fight{fight}.jpg\", image)\n",
        "        fight+=1\n",
        "      else:\n",
        "        cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\non_fight\\nonFight{non_fight}.jpg\", image)\n",
        "        status=\"No fight\"\n",
        "        non_fight+=1\n",
        "      # cv2.putText(image,status, (0,69), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)\n",
        "      # cv2.imshow(\"Hey\",image)\n",
        "      # cv2.waitKey(0)\n",
        "      print(\"frame\"+\" \"+status)\n",
        "      img+=1     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "emupGgLRxjf_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[1. 0.]]\n",
            "corwd1.jpg [[1. 0.]]\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "[[1.000000e+00 2.474153e-32]]\n",
            "crowd.jpg [[1.000000e+00 2.474153e-32]]\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "[[1. 0.]]\n",
            "fight.jpg [[1. 0.]]\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "[[1. 0.]]\n",
            "fight1.jpg [[1. 0.]]\n",
            "1/1 [==============================] - 0s 242ms/step\n",
            "[[1.0000000e+00 2.4135065e-15]]\n",
            "fight3.jpg [[1.0000000e+00 2.4135065e-15]]\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "[[0. 1.]]\n",
            "fight4.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "[[0.60595226 0.39404777]]\n",
            "Fightbend.jpg [[0.60595226 0.39404777]]\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "[[1. 0.]]\n",
            "NONFIGHT.jpg [[1. 0.]]\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "[[2.2916573e-11 1.0000000e+00]]\n",
            "NotFight.jpg [[2.2916573e-11 1.0000000e+00]]\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "[[1.0000000e+00 7.4813714e-24]]\n",
            "simpletalk.jpg [[1.0000000e+00 7.4813714e-24]]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "img=1\n",
        "fight=1\n",
        "non_fight=1\n",
        "video=''\n",
        "for video in os.listdir(r\".\\frames\"):\n",
        "    image = cv2.imread(fr\".\\frames\\{video}\")\n",
        "    if image is not None:\n",
        "        #x = image.img_to_array(img)\n",
        "        # image = image.resize((256, 256))qqqqqq\n",
        "        # image.save('myimage_500.jpg')\n",
        "        # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Check1.jpg\", image)\n",
        "        # image=cv2.resize(image, (256,256))\n",
        "        # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Check2.jpg\", check1)\n",
        "        x = np.expand_dims(image, axis=0)\n",
        "        images = np.vstack([x])\n",
        "        classes = model.predict(images, batch_size=10)\n",
        "        print(classes)\n",
        "        if classes[0][0]>0.5:\n",
        "            status=\"Fighting recorded\"\n",
        "            # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Fight{fight}.jpg\", image)\n",
        "            fight+=1\n",
        "        else:\n",
        "            status=\"No fight\"\n",
        "            # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\non_fight\\nonFight{non_fight}.jpg\", image)\n",
        "            non_fight+=1\n",
        "        cv2.putText(image,status, (0,69), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)\n",
        "        cv2.imshow(\"Hey\",image)\n",
        "        cv2.waitKey(0); cv2.destroyAllWindows()\n",
        "        print(str(video)+\" \"+str(classes))\n",
        "        img+=1   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[[1.9209526e-10 1.0000000e+00]]\n",
            "simpletalk.jpg [[1.9209526e-10 1.0000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread(fr\"C:\\Science\\remains\\Fight\\frame9352.jpg\")\n",
        "if image is not None:\n",
        "    # image = np.asarray(image)\n",
        "    # image = image.resize((256, 256))\n",
        "    # image.save('myimage_500.jpg')\n",
        "    # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Check1.jpg\", image)\n",
        "    # image=cv2.resize(image, (256,256))\n",
        "    # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Check2.jpg\", check1)\n",
        "    cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Check2.jpg\", image)\n",
        "    x = np.expand_dims(image, axis=0)\n",
        "    images = [x]\n",
        "    classes = model.predict(images, batch_size=1)\n",
        "    print(classes)\n",
        "    if classes[0][0]>0.5:\n",
        "        status=\"Fighting recorded\"\n",
        "        # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Fight{fight}.jpg\", image)\n",
        "        fight+=1\n",
        "    else:\n",
        "        status=\"No fight\"\n",
        "        # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\non_fight\\nonFight{non_fight}.jpg\", image)\n",
        "        non_fight+=1\n",
        "    cv2.putText(image,status, (0,69), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)\n",
        "    cv2.imshow(\"Hey\",image)\n",
        "    cv2.waitKey(0); cv2.destroyAllWindows()\n",
        "    print(str(video)+\" \"+str(classes))\n",
        "    img+=1   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "[[0. 1.]]\n",
            "frame10000.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0. 1.]]\n",
            "frame10001.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "[[0. 1.]]\n",
            "frame10002.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0. 1.]]\n",
            "frame10003.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0. 1.]]\n",
            "frame10004.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0. 1.]]\n",
            "frame10005.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "[[0. 1.]]\n",
            "frame10079.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0. 1.]]\n",
            "frame9351.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0. 1.]]\n",
            "frame9352.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0. 1.]]\n",
            "frame9353.jpg [[0. 1.]]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0. 1.]]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [51], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m cv2\u001b[39m.\u001b[39mputText(image,status, (\u001b[39m0\u001b[39m,\u001b[39m69\u001b[39m), cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[39m2\u001b[39m, \u001b[39m255\u001b[39m)\n\u001b[0;32m     29\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mHey\u001b[39m\u001b[39m\"\u001b[39m,image)\n\u001b[1;32m---> 30\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m); cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mstr\u001b[39m(video)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(classes))\n\u001b[0;32m     32\u001b[0m img\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m   \n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "img=1\n",
        "fight=1\n",
        "non_fight=1\n",
        "video=''\n",
        "for video in os.listdir(r\"C:\\Science\\remains\\Fight\"):\n",
        "    image = cv2.imread(fr\"C:\\Science\\remains\\Fight\\{video}\")\n",
        "    if image is not None:\n",
        "        #x = image.img_to_array(img)\n",
        "        # image = image.resize((256, 256))qqqqqq\n",
        "        # image.save('myimage_500.jpg')\n",
        "        # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Check1.jpg\", image)\n",
        "        # image=cv2.resize(image, (256,256))\n",
        "        # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Check2.jpg\", check1)\n",
        "        x = np.expand_dims(image, axis=0)\n",
        "        images = np.vstack([x])\n",
        "        classes = model.predict(images, batch_size=10)\n",
        "        print(classes)\n",
        "        if classes[0][0]>0.5:\n",
        "            status=\"Fighting recorded\"\n",
        "            # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\fight_frames\\Fight{fight}.jpg\", image)\n",
        "            fight+=1\n",
        "        else:\n",
        "            status=\"No fight\"\n",
        "            # cv2.imwrite(fr\"C:\\Science\\Violence-Detection-in-Videos-master\\non_fight\\nonFight{non_fight}.jpg\", image)\n",
        "            non_fight+=1\n",
        "        cv2.putText(image,status, (0,69), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)\n",
        "        cv2.imshow(\"Hey\",image)\n",
        "        cv2.waitKey(0); cv2.destroyAllWindows()\n",
        "        print(str(video)+\" \"+str(classes))\n",
        "        img+=1   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fight"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kaggle All Fight\n",
        "Colab 60%\n",
        "Model 10%\n",
        "Model4-12 All Fight\n",
        "0301Full 40%\n",
        "0310Half 60%\n",
        "SCVD All Non Fight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "559d9de9ffb15158027d2b59b7297d48903e07ab846e9b7423b03d3f59016c92"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
